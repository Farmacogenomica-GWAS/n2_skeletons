{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Results File: L2_skeletons Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "24VwNTVD8ntB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection"
      ],
      "metadata": {
        "id": "qVv2mGNg8pgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlSN9ll28rF2",
        "outputId": "4d2275de-2b40-4212-f7af-7e3f5ff96ffc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "PdvuSBmW80Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "UbY_QPOA81nl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "mTSbBUho81-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/n2_skeletons.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ippVFR84Ja",
        "outputId": "432af493-5982-4352-a8df-bbbcc790c90a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/n2_skeletons.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: blob_features, Size: 23324 elements\n",
            "  Dataset: contour_area, Size: 23324 elements\n",
            "  Dataset: contour_side1, Size: 2285752 elements\n",
            "  Dataset: contour_side1_length, Size: 23324 elements\n",
            "  Dataset: contour_side2, Size: 2285752 elements\n",
            "  Dataset: contour_side2_length, Size: 23324 elements\n",
            "  Dataset: contour_width, Size: 1142876 elements\n",
            "  Dataset: plate_worms, Size: 24866 elements\n",
            "  Dataset: skeleton, Size: 2285752 elements\n",
            "  Dataset: skeleton_length, Size: 23324 elements\n",
            "  Dataset: trajectories_data, Size: 23324 elements\n",
            "  Dataset: width_midbody, Size: 23324 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "Xj4A0TcQ9D6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # IMPORTANT NOTE:\n",
        "# Some reshaping was needed to convert the datasets into a 2D array for its correct extraction\n",
        "# Explanation:\n",
        "# Pandas DataFrames, which are used to create CSV files, require 2-dimensional data.\n",
        "# HDF5 datasets, however, can have any number of dimensions (1D, 2D, 3D, etc.).\n",
        "# When a dataset has more than 2 dimensions, we need to reshape it into a 2D\n",
        "# structure so that it can be stored in a DataFrame and then written to a CSV file."
      ],
      "metadata": {
        "id": "gRikBb1S9Fiv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file (including those within groups)\n",
        "    to individual CSV files. Handles datasets with more than 2 dimensions\n",
        "    by reshaping them. Exports scalar datasets from 'provenance_tracking'\n",
        "    to a separate CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    scalar_data = []\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        def process_hdf5_item(name, obj):\n",
        "            nonlocal scalar_data\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                print(f\"\\nProcessing dataset: {name}\")\n",
        "                if len(obj.shape) > 0:  # Check if the dataset has dimensions\n",
        "                    data = obj[:]  # Read all data\n",
        "\n",
        "                    if len(data.shape) > 2:\n",
        "                        # Reshape the data to 2D\n",
        "                        original_shape = data.shape\n",
        "                        new_shape = (original_shape[0], np.prod(original_shape[1:]))\n",
        "                        data = data.reshape(new_shape)\n",
        "                        print(f\"  Reshaped dataset '{name}' from {original_shape} to {new_shape}\")\n",
        "\n",
        "                    # Convert the data to a Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the output CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{name.replace('/', '_')}.csv\")\n",
        "                    # Replace '/' with '_' in the filename to avoid directory issues\n",
        "\n",
        "                    # Save the DataFrame to a CSV file\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"  Dataset '{name}' successfully exported to: {csv_file_path}\")\n",
        "                elif obj.parent.name == '/provenance_tracking':\n",
        "                    scalar_data.append({'name': name, 'value': obj[()]})\n",
        "                    print(f\"  Found scalar dataset in 'provenance_tracking': {name} = {obj[()]}\")\n",
        "                else:\n",
        "                    print(f\"  Skipping scalar dataset: {name} with shape {obj.shape}\")\n",
        "            elif isinstance(obj, h5py.Group):\n",
        "                print(f\"\\nExploring group: {name}\")\n",
        "                # Recursively process items within the group\n",
        "                obj.visititems(process_hdf5_item)\n",
        "            else:\n",
        "                print(f\"  Skipping non-dataset/group: {name}\")\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            hdf_file.visititems(process_hdf5_item)\n",
        "\n",
        "        # Save scalar data from 'provenance_tracking' to a CSV\n",
        "        if scalar_data:\n",
        "            scalar_df = pd.DataFrame(scalar_data)\n",
        "            scalar_csv_path = os.path.join(output_dir, \"provenance_tracking_scalars.csv\")\n",
        "            scalar_df.to_csv(scalar_csv_path, index=False)\n",
        "            print(f\"\\nScalar datasets from 'provenance_tracking' exported to: {scalar_csv_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/n2_skeletons.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ6lixPM9HV9",
        "outputId": "d2d17d61-d00f-4b64-c624-bf6d7cb96d9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/n2_skeletons.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: blob_features\n",
            "  Dataset 'blob_features' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/blob_features.csv\n",
            "\n",
            "Processing dataset: contour_area\n",
            "  Dataset 'contour_area' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/contour_area.csv\n",
            "\n",
            "Processing dataset: contour_side1\n",
            "  Reshaped dataset 'contour_side1' from (23324, 49, 2) to (23324, np.int64(98))\n",
            "  Dataset 'contour_side1' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/contour_side1.csv\n",
            "\n",
            "Processing dataset: contour_side1_length\n",
            "  Dataset 'contour_side1_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/contour_side1_length.csv\n",
            "\n",
            "Processing dataset: contour_side2\n",
            "  Reshaped dataset 'contour_side2' from (23324, 49, 2) to (23324, np.int64(98))\n",
            "  Dataset 'contour_side2' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/contour_side2.csv\n",
            "\n",
            "Processing dataset: contour_side2_length\n",
            "  Dataset 'contour_side2_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/contour_side2_length.csv\n",
            "\n",
            "Processing dataset: contour_width\n",
            "  Dataset 'contour_width' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/contour_width.csv\n",
            "\n",
            "Exploring group: intensity_analysis\n",
            "\n",
            "Processing dataset: switched_head_tail\n",
            "  Dataset 'switched_head_tail' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/switched_head_tail.csv\n",
            "\n",
            "Processing dataset: intensity_analysis/switched_head_tail\n",
            "  Dataset 'intensity_analysis/switched_head_tail' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/intensity_analysis_switched_head_tail.csv\n",
            "\n",
            "Processing dataset: plate_worms\n",
            "  Dataset 'plate_worms' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/plate_worms.csv\n",
            "\n",
            "Exploring group: provenance_tracking\n",
            "\n",
            "Processing dataset: BLOB_FEATS\n",
            "  Found scalar dataset in 'provenance_tracking': BLOB_FEATS = b'{\"func_name\": \"getBlobsFeats\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"strel_size\\\\\": 5}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: INT_SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': INT_SKE_ORIENT = b'{\"func_name\": \"correctHeadTailIntensity\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"intensities_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_intensities.hdf5\\\\\", \\\\\"smooth_W\\\\\": -1, \\\\\"gap_size\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"local_avg_win\\\\\": -1, \\\\\"min_frac_in\\\\\": 0.85, \\\\\"head_tail_param\\\\\": {\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1}, \\\\\"head_tail_int_method\\\\\": \\\\\"MEDIAN_INT\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_CREATE = b'{\"func_name\": \"trajectories2Skeletons\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"resampling_N\\\\\": 49, \\\\\"worm_midbody\\\\\": [0.33, 0.67], \\\\\"min_blob_area\\\\\": 25, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"skel_args\\\\\": {\\\\\"num_segments\\\\\": 24, \\\\\"head_angle_thresh\\\\\": 60}}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_FILT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_FILT = b'{\"func_name\": \"getFilteredSkels\", \"func_arguments\": \"{\\\\\"bad_seg_thresh\\\\\": 0.8, \\\\\"max_width_ratio\\\\\": 2.25, \\\\\"max_area_ratio\\\\\": 6, \\\\\"min_displacement\\\\\": 10, \\\\\"critical_alpha\\\\\": 0.01, \\\\\"min_num_skel\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_INIT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_INIT = b'{\"func_name\": \"processTrajectoryData\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"smoothed_traj_param\\\\\": {\\\\\"min_track_size\\\\\": 0, \\\\\"displacement_smooth_win\\\\\": -1, \\\\\"threshold_smooth_win\\\\\": -1, \\\\\"roi_size\\\\\": -1}, \\\\\"filter_model_name\\\\\": \\\\\"/home/sirjlister/tierpsy-tracker/tierpsy/extras/model_state_isworm_20200615.pth\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_ORIENT = b'{\"func_name\": \"correctHeadTail\", \"func_arguments\": \"{\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: TRAJ_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': TRAJ_CREATE = b'{\"func_name\": \"getBlobsTable\", \"func_arguments\": \"{\\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"min_area\\\\\": 25, \\\\\"min_box_width\\\\\": 5, \\\\\"worm_bw_thresh_factor\\\\\": 1.05, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"thresh_block_size\\\\\": 61, \\\\\"n_cores_used\\\\\": 1, \\\\\"buffer_size\\\\\": 10}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: TRAJ_JOIN\n",
            "  Found scalar dataset in 'provenance_tracking': TRAJ_JOIN = b'{\"func_name\": \"joinBlobsTrajectories\", \"func_arguments\": \"{\\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"max_allowed_dist\\\\\": 25, \\\\\"min_track_size\\\\\": 0, \\\\\"max_frames_gap\\\\\": 0, \\\\\"area_ratio_lim\\\\\": 2, \\\\\"is_one_worm\\\\\": false, \\\\\"is_WT2\\\\\": false}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/BLOB_FEATS\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/BLOB_FEATS = b'{\"func_name\": \"getBlobsFeats\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"strel_size\\\\\": 5}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/INT_SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/INT_SKE_ORIENT = b'{\"func_name\": \"correctHeadTailIntensity\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"intensities_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_intensities.hdf5\\\\\", \\\\\"smooth_W\\\\\": -1, \\\\\"gap_size\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"local_avg_win\\\\\": -1, \\\\\"min_frac_in\\\\\": 0.85, \\\\\"head_tail_param\\\\\": {\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1}, \\\\\"head_tail_int_method\\\\\": \\\\\"MEDIAN_INT\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_CREATE = b'{\"func_name\": \"trajectories2Skeletons\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"resampling_N\\\\\": 49, \\\\\"worm_midbody\\\\\": [0.33, 0.67], \\\\\"min_blob_area\\\\\": 25, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"skel_args\\\\\": {\\\\\"num_segments\\\\\": 24, \\\\\"head_angle_thresh\\\\\": 60}}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_FILT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_FILT = b'{\"func_name\": \"getFilteredSkels\", \"func_arguments\": \"{\\\\\"bad_seg_thresh\\\\\": 0.8, \\\\\"max_width_ratio\\\\\": 2.25, \\\\\"max_area_ratio\\\\\": 6, \\\\\"min_displacement\\\\\": 10, \\\\\"critical_alpha\\\\\": 0.01, \\\\\"min_num_skel\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_INIT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_INIT = b'{\"func_name\": \"processTrajectoryData\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"smoothed_traj_param\\\\\": {\\\\\"min_track_size\\\\\": 0, \\\\\"displacement_smooth_win\\\\\": -1, \\\\\"threshold_smooth_win\\\\\": -1, \\\\\"roi_size\\\\\": -1}, \\\\\"filter_model_name\\\\\": \\\\\"/home/sirjlister/tierpsy-tracker/tierpsy/extras/model_state_isworm_20200615.pth\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_ORIENT = b'{\"func_name\": \"correctHeadTail\", \"func_arguments\": \"{\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/TRAJ_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/TRAJ_CREATE = b'{\"func_name\": \"getBlobsTable\", \"func_arguments\": \"{\\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/n2.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"min_area\\\\\": 25, \\\\\"min_box_width\\\\\": 5, \\\\\"worm_bw_thresh_factor\\\\\": 1.05, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"thresh_block_size\\\\\": 61, \\\\\"n_cores_used\\\\\": 1, \\\\\"buffer_size\\\\\": 10}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/TRAJ_JOIN\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/TRAJ_JOIN = b'{\"func_name\": \"joinBlobsTrajectories\", \"func_arguments\": \"{\\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/n2_skeletons.hdf5\\\\\", \\\\\"max_allowed_dist\\\\\": 25, \\\\\"min_track_size\\\\\": 0, \\\\\"max_frames_gap\\\\\": 0, \\\\\"area_ratio_lim\\\\\": 2, \\\\\"is_one_worm\\\\\": false, \\\\\"is_WT2\\\\\": false}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: skeleton\n",
            "  Reshaped dataset 'skeleton' from (23324, 49, 2) to (23324, np.int64(98))\n",
            "  Dataset 'skeleton' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/skeleton.csv\n",
            "\n",
            "Processing dataset: skeleton_length\n",
            "  Dataset 'skeleton_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/skeleton_length.csv\n",
            "\n",
            "Exploring group: timestamp\n",
            "\n",
            "Processing dataset: raw\n",
            "  Dataset 'raw' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/raw.csv\n",
            "\n",
            "Processing dataset: time\n",
            "  Dataset 'time' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/time.csv\n",
            "\n",
            "Processing dataset: timestamp/raw\n",
            "  Dataset 'timestamp/raw' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/timestamp_raw.csv\n",
            "\n",
            "Processing dataset: timestamp/time\n",
            "  Dataset 'timestamp/time' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/timestamp_time.csv\n",
            "\n",
            "Processing dataset: trajectories_data\n",
            "  Dataset 'trajectories_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/trajectories_data.csv\n",
            "\n",
            "Processing dataset: width_midbody\n",
            "  Dataset 'width_midbody' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/width_midbody.csv\n",
            "\n",
            "Scalar datasets from 'provenance_tracking' exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets/provenance_tracking_scalars.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of each dataset"
      ],
      "metadata": {
        "id": "MFGd3g2LZPUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Top-Level Datasets"
      ],
      "metadata": {
        "id": "rWAh5GPfZTLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl7aQq57ZV56",
        "outputId": "de16805d-82ab-4274-c094-081cc96c4c5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Datasets\n",
            "==============================\n",
            "\n",
            "--- File: time.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: trajectories_data.csv ---\n",
            "First 5 rows:\n",
            "   frame_number  worm_index_joined  plate_worm_id  skeleton_id    coord_x  \\\n",
            "0             0                  2             16            0  1648.0538   \n",
            "1             1                  2             18            1  1647.4797   \n",
            "2             2                  2             20            2  1647.0740   \n",
            "3             3                  2             22            3  1646.7949   \n",
            "4             4                  2             24            4  1646.6013   \n",
            "\n",
            "     coord_y  threshold  has_skeleton  roi_size   area  timestamp_raw  \\\n",
            "0  1663.1330      128.1             1      68.0  366.5            NaN   \n",
            "1  1662.4630      128.1             1      68.0  366.5            NaN   \n",
            "2  1661.9379      128.1             1      68.0  366.5            NaN   \n",
            "3  1661.5143      128.1             1      68.0  367.0            NaN   \n",
            "4  1661.1490      128.1             1      68.0  367.0            NaN   \n",
            "\n",
            "   timestamp_time  is_good_skel  skel_outliers_flag  int_map_id  \n",
            "0             NaN             1                   0           0  \n",
            "1             NaN             1                   0           1  \n",
            "2             NaN             1                   0           2  \n",
            "3             NaN             1                   0           3  \n",
            "4             NaN             1                   0           4  \n",
            "\n",
            "--- File: width_midbody.csv ---\n",
            "First 5 rows:\n",
            "          0\n",
            "0  6.943441\n",
            "1  6.394460\n",
            "2  6.076469\n",
            "3  6.535634\n",
            "4  6.323060\n",
            "\n",
            "--- File: blob_features.csv ---\n",
            "First 5 rows:\n",
            "     coord_x    coord_y   area  perimeter  box_length  box_width  quirkiness  \\\n",
            "0  1647.8960  1662.7048  368.5  140.12490   54.063095  17.351398    0.947097   \n",
            "1  1648.0614  1662.8177  354.5  138.12490   53.835423  16.643318    0.951013   \n",
            "2  1647.6442  1662.2264  347.0  136.22540   53.534157  16.949924    0.948553   \n",
            "3  1647.1552  1660.7610  359.0  137.53911   51.894090  16.069649    0.950847   \n",
            "4  1647.4644  1660.2554  371.5  136.46803   54.126434  15.504248    0.958097   \n",
            "\n",
            "   compactness  box_orientation  solidity  intensity_mean  intensity_std  \\\n",
            "0     0.235840       -32.125000  0.491333      114.920746       8.030436   \n",
            "1     0.233497       -32.735230  0.494766      114.497580       8.289959   \n",
            "2     0.234976       -32.471190  0.487018      113.791150       8.065038   \n",
            "3     0.238480       -25.016895  0.504923      114.124405       8.527963   \n",
            "4     0.250673       -24.537730  0.520308      113.737816       8.547855   \n",
            "\n",
            "        hu0       hu1       hu2       hu3       hu4       hu5       hu6  \n",
            "0  0.628597  0.311084  0.054412  0.006758 -0.000123 -0.003510 -0.000042  \n",
            "1  0.692604  0.390438  0.054949  0.007130 -0.000118 -0.003758 -0.000077  \n",
            "2  0.694281  0.395342  0.049629  0.006184 -0.000107 -0.003886  0.000016  \n",
            "3  0.641227  0.332233  0.039731  0.005265 -0.000070 -0.002836 -0.000029  \n",
            "4  0.687632  0.386623  0.045736  0.006054 -0.000091 -0.003593  0.000042  \n",
            "\n",
            "--- File: contour_area.csv ---\n",
            "First 5 rows:\n",
            "           0\n",
            "0  368.09763\n",
            "1  353.59286\n",
            "2  346.30800\n",
            "3  358.22060\n",
            "4  370.82867\n",
            "\n",
            "--- File: contour_side1.csv ---\n",
            "First 5 rows:\n",
            "        0       1          2          3          4          5          6  \\\n",
            "0  1641.0  1636.0  1641.9211  1636.9211  1641.1578  1637.8422  1640.2367   \n",
            "1  1640.0  1636.0  1640.9150  1636.9150  1641.8300  1637.8300  1641.9620   \n",
            "2  1640.0  1636.0  1641.0000  1636.3063  1641.0000  1637.6124  1641.0000   \n",
            "3  1643.0  1635.0  1644.0000  1635.2697  1644.0000  1636.5392  1644.0000   \n",
            "4  1642.0  1633.0  1643.2288  1633.2288  1644.0000  1634.2328  1644.0000   \n",
            "\n",
            "           7          8          9  ...         88         89         90  \\\n",
            "0  1638.7633  1640.0000  1639.9679  ...  1664.6178  1678.0000  1665.6509   \n",
            "1  1639.0380  1641.0471  1639.9529  ...  1664.6523  1678.0000  1665.6692   \n",
            "2  1638.9187  1641.0000  1640.2249  ...  1664.6035  1678.0000  1665.9098   \n",
            "3  1637.8088  1644.0000  1639.0784  ...  1661.1161  1677.1161  1662.0197   \n",
            "4  1635.5563  1644.0000  1636.8796  ...  1661.5345  1678.0000  1662.8580   \n",
            "\n",
            "          91         92         93         94         95      96      97  \n",
            "0  1678.6509  1666.8090  1679.0000  1668.0789  1679.0789  1669.0  1680.0  \n",
            "1  1678.6692  1666.8262  1679.0000  1668.0850  1679.0850  1669.0  1680.0  \n",
            "2  1678.0000  1666.8599  1678.8599  1668.0764  1679.0764  1669.0  1680.0  \n",
            "3  1678.0000  1663.2045  1678.2045  1664.1023  1679.1023  1665.0  1680.0  \n",
            "4  1678.0000  1663.8354  1678.8354  1664.7712  1679.7712  1665.0  1681.0  \n",
            "\n",
            "[5 rows x 98 columns]\n",
            "\n",
            "--- File: contour_side1_length.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: contour_side2.csv ---\n",
            "First 5 rows:\n",
            "        0       1          2          3          4          5          6  \\\n",
            "0  1641.0  1636.0  1639.5640  1636.4360  1638.1809  1637.0000  1636.9849   \n",
            "1  1640.0  1636.0  1638.8802  1637.1198  1637.7605  1638.2395  1636.6407   \n",
            "2  1640.0  1636.0  1638.6239  1636.3761  1637.5408  1637.4592  1637.0000   \n",
            "3  1643.0  1635.0  1641.4042  1635.0000  1640.1575  1635.8425  1639.0290   \n",
            "4  1642.0  1633.0  1640.9254  1634.0746  1639.8510  1635.1490  1638.7764   \n",
            "\n",
            "           7          8          9  ...         88      89         90  \\\n",
            "0  1638.0151  1636.0000  1639.2239  ...  1663.9478  1682.0  1665.5643   \n",
            "1  1639.3593  1635.5210  1640.4790  ...  1664.0798  1682.0  1665.6635   \n",
            "2  1638.7670  1636.0815  1639.9185  ...  1663.7012  1682.0  1665.1648   \n",
            "3  1636.9710  1637.9006  1638.0994  ...  1660.4453  1681.0  1661.7361   \n",
            "4  1636.2236  1637.7019  1637.2981  ...  1659.9216  1682.0  1661.4412   \n",
            "\n",
            "          91         92         93         94         95      96      97  \n",
            "0  1682.0000  1667.1809  1682.0000  1668.5640  1681.4360  1669.0  1680.0  \n",
            "1  1682.0000  1667.2471  1682.0000  1668.5874  1681.4126  1669.0  1680.0  \n",
            "2  1681.8352  1666.3506  1681.0000  1667.8824  1681.0000  1669.0  1680.0  \n",
            "3  1681.7361  1663.1575  1681.8425  1664.4042  1681.0000  1665.0  1680.0  \n",
            "4  1682.0000  1662.9608  1682.0000  1664.4803  1682.0000  1665.0  1681.0  \n",
            "\n",
            "[5 rows x 98 columns]\n",
            "\n",
            "--- File: contour_side2_length.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: contour_width.csv ---\n",
            "First 5 rows:\n",
            "              0         1         2         3         4         5         6  \\\n",
            "0  2.220446e-16  1.514141  2.705343  3.418918  4.000000  4.334637  5.084045   \n",
            "1  4.440892e-16  2.903830  4.903830  6.037997  5.272582  5.597760  6.073084   \n",
            "2  6.661338e-16  3.283891  4.000000  4.097282  5.000000  4.584337  5.000000   \n",
            "3  4.002966e-16  2.484978  5.351524  5.656854  5.656854  5.769922  7.295521   \n",
            "4  3.510834e-16  2.273952  4.547904  5.535178  6.520818  7.071068  7.071068   \n",
            "\n",
            "          7         8         9  ...        39        40        41        42  \\\n",
            "0  5.830952  5.252724  5.000000  ...  5.327604  5.000000  5.000000  5.000000   \n",
            "1  5.437798  5.000000  5.000000  ...  6.480062  7.110787  6.757083  5.000000   \n",
            "2  5.981946  6.000000  6.064961  ...  5.148929  5.159641  5.000000  4.408891   \n",
            "3  8.057872  6.030663  4.615166  ...  5.656854  5.612837  4.635588  4.000000   \n",
            "4  7.163813  6.397543  6.630380  ...  7.071068  5.477978  5.064660  5.302421   \n",
            "\n",
            "         43        44        45        46        47   48  \n",
            "0  5.000000  4.228136  3.797764  3.000000  3.000000  0.0  \n",
            "1  4.431147  4.000000  3.665763  3.000000  3.000000  0.0  \n",
            "2  4.000000  4.000000  4.000000  2.802964  1.401482  0.0  \n",
            "3  4.000000  4.000000  3.449555  3.605551  2.222632  0.0  \n",
            "4  5.000000  5.000000  4.000000  4.000000  3.309836  0.0  \n",
            "\n",
            "[5 rows x 49 columns]\n",
            "\n",
            "--- File: switched_head_tail.csv ---\n",
            "First 5 rows:\n",
            "   worm_index  ini_frame  last_frame\n",
            "0           8        191         202\n",
            "1           9          7          14\n",
            "2         124        625         627\n",
            "3         187        543         628\n",
            "4         212        591         622\n",
            "\n",
            "--- File: plate_worms.csv ---\n",
            "First 5 rows:\n",
            "   worm_index_blob  worm_index_joined  frame_number    coord_x    coord_y  \\\n",
            "0                1                  1             0  396.50000  1671.5000   \n",
            "1                1                  1             1  396.00000  1671.5000   \n",
            "2                1                  1             2  395.15045  1671.2434   \n",
            "3                1                  1             3  396.04953  1670.4229   \n",
            "4               30                 30             4  397.06097  1697.9512   \n",
            "\n",
            "   box_length  box_width      angle   area  bounding_box_xmin  \\\n",
            "0   63.000000  29.000000  -0.000000  906.0              382.0   \n",
            "1   63.000000  26.000000  -0.000000  939.0              383.0   \n",
            "2   64.456924  20.421339 -86.185920  788.0              384.0   \n",
            "3   65.109290  23.132270  -0.971022  724.5              384.0   \n",
            "4   20.458763   7.808689 -38.659805   77.0              387.0   \n",
            "\n",
            "   bounding_box_xmax  bounding_box_ymin  bounding_box_ymax  threshold  \n",
            "0              412.0             1640.0             1704.0      147.0  \n",
            "1              410.0             1640.0             1704.0      147.0  \n",
            "2              408.0             1639.0             1704.0      147.0  \n",
            "3              409.0             1638.0             1704.0      147.0  \n",
            "4              407.0             1689.0             1704.0      147.0  \n",
            "\n",
            "--- File: skeleton.csv ---\n",
            "First 5 rows:\n",
            "        0       1          2          3          4          5          6  \\\n",
            "0  1641.0  1636.0  1640.3229  1637.3542  1639.2078  1638.0000  1638.6936   \n",
            "1  1640.0  1636.0  1640.0000  1637.4519  1640.0000  1638.9038  1639.0414   \n",
            "2  1640.0  1636.0  1639.7161  1637.2839  1639.0000  1638.3888  1639.0000   \n",
            "3  1643.0  1635.0  1643.0000  1636.3784  1642.4648  1637.5352  1641.4901   \n",
            "4  1642.0  1633.0  1642.4548  1634.3644  1642.9095  1635.7288  1642.1852   \n",
            "\n",
            "           7          8          9  ...         88      89         90  \\\n",
            "0  1639.0000  1638.0000  1639.8206  ...  1663.7719  1680.0  1665.2023   \n",
            "1  1639.9586  1639.0000  1641.3934  ...  1664.0208  1680.0  1665.3342   \n",
            "2  1639.7903  1638.8644  1641.1356  ...  1663.3940  1680.0  1664.7955   \n",
            "3  1638.5099  1640.5155  1639.4845  ...  1660.5508  1679.0  1661.6571   \n",
            "4  1636.8148  1642.0000  1638.1761  ...  1659.6615  1680.0  1661.0997   \n",
            "\n",
            "          91         92         93         94         95      96      97  \n",
            "0  1680.2023  1666.3860  1681.0000  1667.9000  1681.0000  1669.0  1680.0  \n",
            "1  1680.3342  1666.5104  1681.0000  1667.9623  1681.0000  1669.0  1680.0  \n",
            "2  1680.0000  1666.1970  1680.0000  1667.5985  1680.0000  1669.0  1680.0  \n",
            "3  1679.6571  1662.6317  1680.6317  1663.7671  1680.6165  1665.0  1680.0  \n",
            "4  1680.0000  1662.5378  1680.0000  1663.6902  1680.6902  1665.0  1681.0  \n",
            "\n",
            "[5 rows x 98 columns]\n",
            "\n",
            "--- File: skeleton_length.csv ---\n",
            "First 5 rows:\n",
            "           0\n",
            "0  72.678764\n",
            "1  69.691910\n",
            "2  67.271126\n",
            "3  66.164050\n",
            "4  69.032330\n",
            "\n",
            "--- File: raw.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Datasets extracted from the groups"
      ],
      "metadata": {
        "id": "OKpbArRhZchv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Groups'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIYiev_JZgAr",
        "outputId": "9362d15b-37c5-48c2-ee2c-b27e10121e24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/n2/n2_skeletons/Groups\n",
            "==============================\n",
            "\n",
            "--- File: timestamp_raw.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: timestamp_time.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: intensity_analysis_switched_head_tail.csv ---\n",
            "First 5 rows:\n",
            "   worm_index  ini_frame  last_frame\n",
            "0           8        191         202\n",
            "1           9          7          14\n",
            "2         124        625         627\n",
            "3         187        543         628\n",
            "4         212        591         622\n",
            "\n",
            "--- File: provenance_tracking_scalars.csv ---\n",
            "First 5 rows:\n",
            "             name                                              value\n",
            "0      BLOB_FEATS  b'{\"func_name\": \"getBlobsFeats\", \"func_argumen...\n",
            "1  INT_SKE_ORIENT  b'{\"func_name\": \"correctHeadTailIntensity\", \"f...\n",
            "2      SKE_CREATE  b'{\"func_name\": \"trajectories2Skeletons\", \"fun...\n",
            "3        SKE_FILT  b'{\"func_name\": \"getFilteredSkels\", \"func_argu...\n",
            "4        SKE_INIT  b'{\"func_name\": \"processTrajectoryData\", \"func...\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    }
  ]
}